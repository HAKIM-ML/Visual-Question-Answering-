{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8107969,"sourceType":"datasetVersion","datasetId":4789129}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Visual Question Answering\n\nThis code is designed to build and train a Visual Question Answering (VQA) model using Keras and TensorFlow. The model takes an image and a question as input and predicts the answer to the question based on the image content.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, VGG16\nfrom tqdm import tqdm  # Import tqdm for the progress bar\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten, Concatenate,GlobalAveragePooling2D, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show keras","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:31:14.277208Z","iopub.execute_input":"2024-04-15T06:31:14.278107Z","iopub.status.idle":"2024-04-15T06:31:25.930998Z","shell.execute_reply.started":"2024-04-15T06:31:14.278065Z","shell.execute_reply":"2024-04-15T06:31:25.929873Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Name: keras\nVersion: 3.1.1\nSummary: Multi-backend Keras.\nHome-page: https://github.com/keras-team/keras\nAuthor: Keras team\nAuthor-email: keras-users@googlegroups.com\nLicense: Apache License 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: absl-py, h5py, ml-dtypes, namex, numpy, optree, rich\nRequired-by: keras-tuner, tensorflow\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Data\n\nit defines a function load_data that reads the CSV files containing the training and evaluation data, including questions, answers, and image IDs. This function loads the images, preprocesses them using the VGG16 model, and returns the questions, answers, and preprocessed images.","metadata":{}},{"cell_type":"code","source":"def load_data(csv_file, image_dir):\n    data = pd.read_csv(csv_file)\n    questions = data['question']\n    answers = data['answer']\n\n    images = []\n    for fname in tqdm(data['image_id'], desc=\"Loading images\"):\n        img_path = os.path.join(image_dir, f\"{fname}.png\")\n        img = load_img(img_path, target_size=(224, 224))\n        img_array = img_to_array(img)\n        images.append(img_array)\n\n    images = preprocess_input(np.array(images))\n    return questions, answers, images\n\n# Example usage\ndata_path = \"/kaggle/input/visual-question-answering-dataset\"\ntrain_data = os.path.join(data_path, \"data_train.csv\")\neval_data = os.path.join(data_path, \"data_eval.csv\")\nimage_dir = os.path.join(data_path, \"images\")\n\ntrain_questions, train_answers, train_images = load_data(train_data, image_dir)\neval_questions, eval_answers, eval_images = load_data(eval_data, image_dir)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:18:26.739255Z","iopub.execute_input":"2024-04-15T06:18:26.739642Z","iopub.status.idle":"2024-04-15T06:20:57.200175Z","shell.execute_reply.started":"2024-04-15T06:18:26.739609Z","shell.execute_reply":"2024-04-15T06:20:57.199142Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Loading images: 100%|██████████| 9974/9974 [02:00<00:00, 82.70it/s] \nLoading images: 100%|██████████| 2494/2494 [00:25<00:00, 98.44it/s] \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenization\n\nthe code preprocesses the text data (questions) by tokenizing the words, converting them to sequences of integers, and padding the sequences to a fixed length.\n\nAfter that, it encodes the answer labels by creating a mapping dictionary from labels to integers, replacing the text labels with their corresponding integers, and converting the integer-encoded labels to a one-hot encoded format suitable for training the neural network.","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_questions)\n\n# Convert text to sequences\ntrain_seq = tokenizer.texts_to_sequences(train_questions)\neval_seq = tokenizer.texts_to_sequences(eval_questions)\n\n# Pad sequences\nmax_len = max(len(x) for x in train_seq)\ntrain_seq = pad_sequences(train_seq, maxlen=max_len)\neval_seq = pad_sequences(eval_seq, maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:20:57.731289Z","iopub.execute_input":"2024-04-15T06:20:57.731997Z","iopub.status.idle":"2024-04-15T06:20:58.091061Z","shell.execute_reply.started":"2024-04-15T06:20:57.731961Z","shell.execute_reply":"2024-04-15T06:20:58.090093Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Get the unique answer labels from both training and evaluation sets\nall_answers = train_answers.tolist() + eval_answers.tolist()\nunique_answers = list(set(all_answers))\n\n# Create a mapping dictionary from labels to integers\nlabel_to_int = {label: i for i, label in enumerate(unique_answers)}\n\n# Replace text labels with integer labels\ntrain_answers_encoded = [label_to_int[label] for label in train_answers]\neval_answers_encoded = [label_to_int[label] for label in eval_answers]\n\n# Update the value of num_classes based on the new unique_answers list\nnum_classes = len(unique_answers)\n\n# Convert to categorical\ntrain_answers_categorical = to_categorical(train_answers_encoded, num_classes=num_classes)\neval_answers_categorical = to_categorical(eval_answers_encoded, num_classes=num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:40:32.952370Z","iopub.execute_input":"2024-04-15T06:40:32.953075Z","iopub.status.idle":"2024-04-15T06:40:33.041961Z","shell.execute_reply.started":"2024-04-15T06:40:32.953046Z","shell.execute_reply":"2024-04-15T06:40:33.041104Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture\n\nThe code then defines the architecture of the VQA model, which consists of two branches: one for processing the image input and another for processing the text input (questions). The image branch uses the pre-trained VGG16 model to extract features, while the text branch uses an Embedding layer, Bidirectional LSTM, and LSTM layers to process the question sequences. The outputs from both branches are concatenated and passed through dense layers to produce the final output probabilities for each answer class.\n\nThe model is compiled with the Adam optimizer and categorical cross-entropy loss function, and the accuracy metric is specified for evaluation.\n\nAfter defining the model, the code trains it using the preprocessed training data and saves the trained model to a file named 'vaq.h5'.","metadata":{}},{"cell_type":"code","source":"# Image model\nimage_input = Input(shape=(224, 224, 3))\nimage_model = VGG16(include_top=False, weights='imagenet')(image_input)\nimage_model = Flatten()(image_model)\n\n# Text Model\ntext_input = Input(shape=(max_len,))\ntext_model = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100)(text_input)\ntext_model = Bidirectional(LSTM(256, return_sequences=True))(text_model)\ntext_model = LSTM(512)(text_model)\n\n# Concatenate models\ncombined = Concatenate()([image_model, text_model])\ncombined = Dense(256, activation='relu')(combined)\noutput = Dense(num_classes, activation='softmax')(combined)\n\nmodel = Model(inputs=[image_input, text_input], outputs=output)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:40:33.818856Z","iopub.execute_input":"2024-04-15T06:40:33.819557Z","iopub.status.idle":"2024-04-15T06:40:34.346089Z","shell.execute_reply.started":"2024-04-15T06:40:33.819529Z","shell.execute_reply":"2024-04-15T06:40:34.345335Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel.fit(\n    [train_images, train_seq],\n    train_answers_categorical,\n    epochs=1,\n    batch_size=8,\n    verbose=1\n)\nmodel.save('vaq.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:40:45.903193Z","iopub.execute_input":"2024-04-15T06:40:45.903857Z","iopub.status.idle":"2024-04-15T06:44:39.291372Z","shell.execute_reply.started":"2024-04-15T06:40:45.903825Z","shell.execute_reply":"2024-04-15T06:44:39.290047Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\u001b[1m1247/1247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 160ms/step - accuracy: 0.0403 - loss: 5.9966\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      3\u001b[0m     [train_images, train_seq],\n\u001b[1;32m      4\u001b[0m     train_answers_categorical,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvaq.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[0;32m--> 163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m    166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n","File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/h5d.pyx:137\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unable to synchronously create dataset (name already exists)"],"ename":"ValueError","evalue":"Unable to synchronously create dataset (name already exists)","output_type":"error"}]},{"cell_type":"code","source":"print(\"num_classes:\", num_classes)\nprint(\"len(unique_answers):\", len(unique_answers))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_answers = list(train_answers.tolist() + eval_answers.tolist())\nunique_answers_check = list(set(all_answers))\nprint(\"Unique answers (check):\", len(unique_answers_check))\n\nif len(unique_answers_check) != len(unique_answers):\n    print(\"Warning: unique_answers list may be incomplete!\")\n    \n    \nfor label, idx in label_to_int.items():\n    if idx < 0 or idx >= len(unique_answers):\n        print(f\"Warning: Label '{label}' has incorrect index {idx}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T08:02:48.430366Z","iopub.execute_input":"2024-04-14T08:02:48.430761Z","iopub.status.idle":"2024-04-14T08:02:48.438836Z","shell.execute_reply.started":"2024-04-14T08:02:48.430728Z","shell.execute_reply":"2024-04-14T08:02:48.437913Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Unique answers (check): 1443\n","output_type":"stream"}]},{"cell_type":"code","source":"import joblib\n\n# Save preprocess_input\njoblib.dump(preprocess_input, 'preprocesse_input.joblib')\n\n# Save unique_answers\njoblib.dump(unique_answers, 'unique_answers.joblib')\njoblib.dump(tokenizer, 'tokenizer.joblib')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:23:20.643082Z","iopub.execute_input":"2024-04-15T06:23:20.643777Z","iopub.status.idle":"2024-04-15T06:23:20.749783Z","shell.execute_reply.started":"2024-04-15T06:23:20.643744Z","shell.execute_reply":"2024-04-15T06:23:20.748894Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['tokenizer.joblib']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Inference","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, VGG16\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten, Concatenate\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.utils import to_categorical\n\n\n# model = load_model('/kaggle/working/model.h5')\n# Function to preprocess the image\ndef preprocess_image(image_path):\n    img = load_img(image_path, target_size=(224, 224))\n    img_array = img_to_array(img)\n    img_array = preprocess_input(np.array([img_array]))\n    return img_array\n\n# Function to preprocess the question\ndef preprocess_question(question):\n    seq = tokenizer.texts_to_sequences([question])\n    seq = pad_sequences(seq, maxlen=max_len)\n    return seq\n\n# Function to predict the answer\ndef predict_answer(image_path, question):\n    preprocessed_image = preprocess_image(image_path)\n    preprocessed_question = preprocess_question(question)\n\n    prediction = model.predict([preprocessed_image, preprocessed_question])\n    predicted_class_idx = np.argmax(prediction)\n    predicted_answer = unique_answers[predicted_class_idx]\n\n    return predicted_answer\n\n# Example usage\nwhile True:\n    image_path = input(\"Enter the image path: \")\n    question = input(\"Enter the question: \")\n\n    predicted_answer = predict_answer(image_path, question)\n    print(f\"Predicted answer: {predicted_answer}\")\n\n    continue_prompt = input(\"Do you want to continue? (y/n) \")\n    if continue_prompt.lower() != 'y':\n        break","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:05:18.668878Z","iopub.execute_input":"2024-04-14T19:05:18.669266Z","iopub.status.idle":"2024-04-14T19:05:42.180027Z","shell.execute_reply.started":"2024-04-14T19:05:18.669236Z","shell.execute_reply":"2024-04-14T19:05:42.179150Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the image path:  /kaggle/input/visual-question-answering-dataset/images/image100.png\nEnter the question:  what is the object on the shelves\n"},{"name":"stdout","text":"1/1 [==============================] - 3s 3s/step\nPredicted answer: cup\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to continue? (y/n)  n\n"}]},{"cell_type":"code","source":"model.save('vaq.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:34:39.183381Z","iopub.execute_input":"2024-04-15T06:34:39.184247Z","iopub.status.idle":"2024-04-15T06:34:39.190044Z","shell.execute_reply.started":"2024-04-15T06:34:39.184212Z","shell.execute_reply":"2024-04-15T06:34:39.189029Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"24"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}